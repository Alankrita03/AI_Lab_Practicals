{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwGQZbMWsOG6",
        "outputId": "8a069ad2-3dfa-41b6-bc61-f532f9fec543"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document 1:\n",
            "Original text: This is a sample paragraph. It contains several se...\n",
            "Tokens (first 10): ['this', 'is', 'a', 'sample', 'paragraph', '.', 'it', 'contains', 'several', 'sentences']\n",
            "After removing stopwords (first 10): ['sample', 'paragraph', 'contains', 'several', 'sentences', 'different', 'words']\n",
            "After stemming (first 10): ['sampl', 'paragraph', 'contain', 'sever', 'sentenc', 'differ', 'word']\n",
            "\n",
            "Document 2:\n",
            "Original text: Natural language processing is a field of artifici...\n",
            "Tokens (first 10): ['natural', 'language', 'processing', 'is', 'a', 'field', 'of', 'artificial', 'intelligence', '.']\n",
            "After removing stopwords (first 10): ['natural', 'language', 'processing', 'field', 'artificial', 'intelligence']\n",
            "After stemming (first 10): ['natur', 'languag', 'process', 'field', 'artifici', 'intellig']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "#Data Preprocessing\n",
        "\n",
        "%pip install nltk\n",
        "%pip install pandas\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "# Create our text processing function\n",
        "def process_text(text):\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
        "\n",
        "    # Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
        "\n",
        "    return tokens, filtered_tokens, stemmed_tokens\n",
        "\n",
        "\n",
        "# CHOOSE ONE OF THESE OPTIONS TO LOAD YOUR DATA:\n",
        "\n",
        "\n",
        "# OPTION 1: For CSV files (uncomment the lines below)\n",
        "# df = pd.read_csv('your_dataset.csv')\n",
        "# texts = df['text'].tolist()  # Change 'text' to your column name\n",
        "\n",
        "\n",
        "# OPTION 2: For Excel files (uncomment the lines below)\n",
        "# df = pd.read_excel('your_dataset.xlsx')\n",
        "# texts = df['text'].tolist()  # Change 'text' to your column name\n",
        "\n",
        "\n",
        "# OPTION 3: For text files (uncomment the lines below)\n",
        "# with open('your_dataset.txt', 'r', encoding='utf-8') as file:\n",
        "#     texts = [file.read()]\n",
        "\n",
        "\n",
        "# OPTION 4: For paragraphs directly (uncomment the lines below)\n",
        "texts = [\n",
        "    \"This is a sample paragraph. It contains several sentences with different words.\",\n",
        "    \"Natural language processing is a field of artificial intelligence.\"\n",
        "]\n",
        "\n",
        "\n",
        "# Process each text\n",
        "for i, text in enumerate(texts):\n",
        "    print(f\"\\nDocument {i+1}:\")\n",
        "\n",
        "    # Process the text\n",
        "    tokens, filtered_tokens, stemmed_tokens = process_text(text)\n",
        "\n",
        "    # Print results\n",
        "    print(\"Original text:\", text[:50] + \"...\" if len(text) > 50 else text)\n",
        "    print(\"Tokens (first 10):\", tokens[:10])\n",
        "    print(\"After removing stopwords (first 10):\", filtered_tokens[:10])\n",
        "    print(\"After stemming (first 10):\", stemmed_tokens[:10])"
      ]
    }
  ]
}